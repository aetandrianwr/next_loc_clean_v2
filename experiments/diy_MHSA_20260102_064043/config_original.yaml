# Hyperparameter Tuning Config: mhsa_diy_trial05
# Model: mhsa, Dataset: diy, Trial: 5
# Hyperparameters: {'base_emb_size': 64, 'num_encoder_layers': 3, 'nhead': 8, 'dim_feedforward': 128, 'fc_dropout': 0.25, 'lr': 0.0005, 'weight_decay': 0.0001, 'batch_size': 32, 'num_warmup_epochs': 2, 'trial_idx': 5, 'model_name': 'mhsa', 'dataset': 'diy'}

seed: 42
data:
  data_dir: data/diy_eps50/processed
  dataset_prefix: diy_eps50_prev7
  dataset: diy
  experiment_root: experiments
training:
  if_embed_user: true
  if_embed_poi: false
  if_embed_time: true
  if_embed_duration: true
  previous_day: 7
  verbose: true
  debug: false
  batch_size: 32
  print_step: 50
  num_workers: 0
  day_selection: default
dataset_info:
  total_loc_num: 7038
  total_user_num: 693
embedding:
  base_emb_size: 64
  poi_original_size: 16
model:
  networkName: transformer
  num_encoder_layers: 3
  nhead: 8
  dim_feedforward: 128
  fc_dropout: 0.25
optimiser:
  optimizer: Adam
  max_epoch: 100
  lr: 0.0005
  weight_decay: 0.0001
  beta1: 0.9
  beta2: 0.999
  momentum: 0.98
  num_warmup_epochs: 2
  num_training_epochs: 50
  patience: 5
  lr_step_size: 1
  lr_gamma: 0.1
