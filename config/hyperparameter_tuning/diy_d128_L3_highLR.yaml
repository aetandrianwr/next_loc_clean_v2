# DIY Configuration - Higher LR with baseline architecture
# d_model=128, num_layers=3, dim_feedforward=256
# Expected params: ~2.4M (limit: 3M)
# Hypothesis: Baseline architecture with higher LR

seed: 42

data:
  data_dir: data/diy_eps50/processed
  dataset_prefix: diy_eps50_prev7
  dataset: diy
  experiment_root: experiments
  num_workers: 0

model:
  d_model: 128
  nhead: 4
  num_layers: 3
  dim_feedforward: 256
  dropout: 0.15

training:
  batch_size: 128
  num_epochs: 50
  learning_rate: 0.0009
  weight_decay: 0.015
  label_smoothing: 0.03
  grad_clip: 0.8
  patience: 5
  min_epochs: 8
  warmup_epochs: 5
  use_amp: true
  min_lr: 0.000001
