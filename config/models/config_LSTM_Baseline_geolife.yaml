# LSTM Baseline Model Configuration for GeoLife Dataset
# Dataset: geolife_eps20
#
# Expected performance based on Hong et al. 2023:
#   - Acc@1: ~28.4%
#   - Acc@5: ~55.8%
#   - Acc@10: ~59.1%
#   - MRR: ~40.2%
#   - NDCG@10: ~44.7%
#
# This configuration follows the paper's baseline setup:
#   - Same input features as MHSA (location, time, weekday, duration, user)
#   - Batch size of 32 for Geolife (as per paper)
#   - Embedding dimension of 32 (as per paper Table S1)
#
# Usage:
#   python src/training/train_LSTM_Baseline.py --config config/models/config_LSTM_Baseline_geolife.yaml

seed: 42

# Data settings
data:
  data_dir: data/geolife_eps20/processed
  dataset_prefix: geolife_eps20_prev7
  dataset: geolife
  experiment_root: experiments

# Training settings
training:
  if_embed_user: true
  if_embed_poi: false
  if_embed_time: true
  if_embed_duration: true
  
  previous_day: 7
  verbose: true
  debug: false
  batch_size: 32
  print_step: 20
  num_workers: 0
  day_selection: default

# Dataset info (from metadata)
dataset_info:
  total_loc_num: 1187
  total_user_num: 46

# Embedding settings (matching paper Table S1 for Geolife)
embedding:
  base_emb_size: 32

# Model architecture
# LSTM baseline following the paper's setup
# Paper reports ~147K parameters for LSTM on Geolife
model:
  hidden_size: 64
  num_layers: 2
  lstm_dropout: 0.2
  fc_dropout: 0.2

# Optimizer settings (matching MHSA training setup for fair comparison)
optimiser:
  optimizer: Adam
  max_epoch: 100
  lr: 0.001
  weight_decay: 0.000001
  beta1: 0.9
  beta2: 0.999
  momentum: 0.98
  num_warmup_epochs: 2
  num_training_epochs: 50
  patience: 5
  lr_step_size: 1
  lr_gamma: 0.1
