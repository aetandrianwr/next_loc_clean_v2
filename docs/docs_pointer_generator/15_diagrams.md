# Visual Diagrams and Flowcharts

This document consolidates all major diagrams for the pointer-generator network.

## Table of Contents
1. [Architecture Overview](#architecture-overview)
2. [Data Flow](#data-flow)
3. [Encoder Architecture](#encoder-architecture)
4. [Attention Mechanism](#attention-mechanism)
5. [Pointer-Generator](#pointer-generator)
6. [Coverage Mechanism](#coverage-mechanism)
7. [Training Pipeline](#training-pipeline)
8. [Beam Search](#beam-search)

---

## Architecture Overview

```
┌──────────────────────────────────────────────────────────────────────────────────────────────┐
│                           POINTER-GENERATOR NETWORK ARCHITECTURE                              │
├──────────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                               │
│   INPUT                              ENCODER                                                  │
│   ─────                              ───────                                                  │
│                                                                                               │
│   "Germany beat Argentina"           ┌────────────────────────────────────┐                  │
│         │                            │      BIDIRECTIONAL LSTM             │                  │
│         ▼                            │                                     │                  │
│   ┌─────────────┐                    │   →→→→→→→→→→→→→→→                  │                  │
│   │ Embedding   │ ──────────────────▶│   h₁  h₂  h₃  h₄  h₅              │                  │
│   │   Layer     │                    │   ←←←←←←←←←←←←←←←                  │                  │
│   └─────────────┘                    │                                     │                  │
│                                      └──────────┬─────────────────────────┘                  │
│                                                 │                                             │
│                          ┌──────────────────────┼──────────────────────────┐                 │
│                          │                      │                          │                 │
│                          ▼                      ▼                          ▼                 │
│                    ┌──────────┐          ┌──────────┐               ┌──────────┐            │
│                    │ Encoder  │          │ Encoder  │               │ Reduce   │            │
│                    │ Output 1 │          │ Output 2 │     ...       │ States   │            │
│                    │ [512]    │          │ [512]    │               │ → [256]  │            │
│                    └────┬─────┘          └────┬─────┘               └────┬─────┘            │
│                         │                     │                          │                   │
│                         │                     │                          │                   │
│                         └─────────────────────┼──────────────────────────┘                   │
│                                               │                                              │
│                                               ▼                                              │
│                                        DECODER                                               │
│                                        ───────                                               │
│                                                                                              │
│   ┌──────────────────────────────────────────────────────────────────────────────────┐      │
│   │                                                                                   │      │
│   │                              ┌──────────────┐                                     │      │
│   │                              │   Attention  │                                     │      │
│   │         ┌────────────────────│   Mechanism  │◀───────────────────┐               │      │
│   │         │                    │              │                    │               │      │
│   │         │                    └──────┬───────┘                    │               │      │
│   │         │                           │                            │               │      │
│   │         │ context                   │ attention                  │               │      │
│   │         │ vector                    │ weights                    │               │      │
│   │         ▼                           ▼                            │               │      │
│   │   ┌──────────┐              ┌──────────────┐              ┌──────────┐           │      │
│   │   │   LSTM   │              │   p_gen      │              │ Decoder  │           │      │
│   │   │  Decoder │───────┬─────▶│ Calculator   │              │  State   │───────────┘      │
│   │   │          │       │      │              │              │          │                  │
│   │   └────┬─────┘       │      └──────┬───────┘              └──────────┘                  │
│   │        │             │             │                                                    │
│   │        │             │             │ p_gen                                              │
│   │        ▼             │             ▼                                                    │
│   │   ┌──────────┐       │      ┌──────────────────────────────────────┐                   │
│   │   │  Output  │       │      │         FINAL DISTRIBUTION           │                   │
│   │   │Projection│───────┴─────▶│                                      │                   │
│   │   │          │              │  P(w) = p_gen × P_vocab(w)           │                   │
│   │   └──────────┘              │       + (1-p_gen) × P_copy(w)        │                   │
│   │        │                    │                                      │                   │
│   │        │                    └──────────────────────────────────────┘                   │
│   │        │                                     │                                          │
│   │        │ vocab                               │                                          │
│   │        │ distribution                        │ final                                    │
│   │        ▼                                     ▼ distribution                             │
│   │                                                                                         │
│   └─────────────────────────────────────────────────────────────────────────────────────────┘
│                                                                                               │
│   OUTPUT: "Germany won the Cup"                                                              │
│                                                                                               │
└──────────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Data Flow

```
┌──────────────────────────────────────────────────────────────────────────────────────────────┐
│                               DATA FLOW DIAGRAM                                               │
├──────────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                               │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                           RAW DATA                                                   │    │
│   │                                                                                      │    │
│   │   Article: "Elon Musk announced Tesla's new battery"                                │    │
│   │   Abstract: "<s> Musk unveiled Tesla battery </s>"                                  │    │
│   │                                                                                      │    │
│   └───────────────────────────────────┬─────────────────────────────────────────────────┘    │
│                                       │                                                       │
│                                       ▼                                                       │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                         VOCABULARY LOOKUP                                            │    │
│   │                                                                                      │    │
│   │   vocab = {announced: 100, new: 200, battery: 300, unveiled: 400, ...}             │    │
│   │   OOV = [Elon, Musk, Tesla]                                                         │    │
│   │                                                                                      │    │
│   └───────────────────────────────────┬─────────────────────────────────────────────────┘    │
│                                       │                                                       │
│                                       ▼                                                       │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                          DUAL ENCODING                                               │    │
│   │                                                                                      │    │
│   │   enc_input:            [1, 1, 100, 1, 's, 200, 300]                               │    │
│   │                          ↑  ↑      ↑                                                │    │
│   │                        Elon Musk Tesla → UNK                                        │    │
│   │                                                                                      │    │
│   │   enc_input_extend:     [50000, 50001, 100, 50002, 's, 200, 300]                   │    │
│   │                          ↑      ↑           ↑                                       │    │
│   │                        Elon   Musk        Tesla → temp IDs                         │    │
│   │                                                                                      │    │
│   │   article_oovs:         ["Elon", "Musk", "Tesla"]                                  │    │
│   │                                                                                      │    │
│   └───────────────────────────────────┬─────────────────────────────────────────────────┘    │
│                                       │                                                       │
│                                       ▼                                                       │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                            ENCODER                                                   │    │
│   │                                                                                      │    │
│   │   enc_input ──▶ [Embedding] ──▶ [BiLSTM] ──▶ encoder_outputs [7, 512]             │    │
│   │                                         └──▶ dec_init_state (c, h)                 │    │
│   │                                                                                      │    │
│   └───────────────────────────────────┬─────────────────────────────────────────────────┘    │
│                                       │                                                       │
│                                       ▼                                                       │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                            DECODER (per step)                                        │    │
│   │                                                                                      │    │
│   │   dec_input ──▶ [Embedding] ──┐                                                     │    │
│   │                               ├──▶ [LSTM Cell] ──▶ decoder_output                   │    │
│   │   prev_context ───────────────┘         │                                           │    │
│   │                                         ▼                                           │    │
│   │                                    [Attention]                                      │    │
│   │                                         │                                           │    │
│   │                          ┌──────────────┼──────────────┐                            │    │
│   │                          │              │              │                            │    │
│   │                          ▼              ▼              ▼                            │    │
│   │                     attn_dist      context_vec     coverage                         │    │
│   │                                                                                      │    │
│   └───────────────────────────────────┬─────────────────────────────────────────────────┘    │
│                                       │                                                       │
│                                       ▼                                                       │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                       OUTPUT DISTRIBUTION                                            │    │
│   │                                                                                      │    │
│   │   decoder_output ──▶ [Output Proj] ──▶ vocab_dist [50000]                          │    │
│   │                                              │                                       │    │
│   │                                              ▼                                       │    │
│   │   p_gen × vocab_dist + (1-p_gen) × attn_dist ──▶ final_dist [50000 + max_oovs]    │    │
│   │                                                                                      │    │
│   └───────────────────────────────────┬─────────────────────────────────────────────────┘    │
│                                       │                                                       │
│                                       ▼                                                       │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                            OUTPUT                                                    │    │
│   │                                                                                      │    │
│   │   argmax(final_dist) ──▶ output_id ──▶ [Vocab/OOV lookup] ──▶ "Musk"              │    │
│   │                                                                                      │    │
│   └─────────────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                               │
└──────────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Encoder Architecture

```
┌──────────────────────────────────────────────────────────────────────────────────────────────┐
│                            BIDIRECTIONAL LSTM ENCODER                                         │
├──────────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                               │
│   Input: "Germany beat Argentina in the final"                                               │
│                                                                                               │
│   EMBEDDING LAYER:                                                                            │
│   ─────────────────                                                                           │
│                                                                                               │
│   Word:       Germany    beat    Argentina    in      the     final                          │
│   ID:           100       101       102       6        4       105                           │
│                  │         │         │        │        │        │                            │
│                  ▼         ▼         ▼        ▼        ▼        ▼                            │
│   Embed:     [e_100]   [e_101]   [e_102]   [e_6]    [e_4]   [e_105]                         │
│              [128]     [128]     [128]     [128]    [128]    [128]                          │
│                                                                                               │
│                                                                                               │
│   FORWARD LSTM:                                                                               │
│   ─────────────                                                                               │
│                                                                                               │
│   ─────────────────────────────────────────────────────────────────────────▶                 │
│                                                                                               │
│     init ──▶ LSTM ──▶ LSTM ──▶ LSTM ──▶ LSTM ──▶ LSTM ──▶ LSTM ──▶ final                   │
│              │         │         │         │         │         │    state                    │
│              ▼         ▼         ▼         ▼         ▼         ▼                             │
│            h→_0      h→_1      h→_2      h→_3      h→_4      h→_5                           │
│           [256]     [256]     [256]     [256]     [256]     [256]                           │
│                                                                                               │
│                                                                                               │
│   BACKWARD LSTM:                                                                              │
│   ──────────────                                                                              │
│                                                                                               │
│   ◀─────────────────────────────────────────────────────────────────────────                 │
│                                                                                               │
│   final ◀── LSTM ◀── LSTM ◀── LSTM ◀── LSTM ◀── LSTM ◀── LSTM ◀── init                    │
│   state      │         │         │         │         │         │                             │
│              ▼         ▼         ▼         ▼         ▼         ▼                             │
│            h←_0      h←_1      h←_2      h←_3      h←_4      h←_5                           │
│           [256]     [256]     [256]     [256]     [256]     [256]                           │
│                                                                                               │
│                                                                                               │
│   CONCATENATION:                                                                              │
│   ──────────────                                                                              │
│                                                                                               │
│            h→_0      h→_1      h→_2      h→_3      h→_4      h→_5                           │
│              +         +         +         +         +         +                             │
│            h←_0      h←_1      h←_2      h←_3      h←_4      h←_5                           │
│              │         │         │         │         │         │                             │
│              ▼         ▼         ▼         ▼         ▼         ▼                             │
│            h_0       h_1       h_2       h_3       h_4       h_5                             │
│           [512]     [512]     [512]     [512]     [512]     [512]                           │
│                                                                                               │
│   encoder_outputs: [batch=1, seq_len=6, hidden=512]                                          │
│                                                                                               │
│                                                                                               │
│   STATE REDUCTION:                                                                            │
│   ────────────────                                                                            │
│                                                                                               │
│      fw_state.c [256]  +  bw_state.c [256]  ──▶  concat [512]  ──▶  W_c  ──▶  dec_c [256]  │
│      fw_state.h [256]  +  bw_state.h [256]  ──▶  concat [512]  ──▶  W_h  ──▶  dec_h [256]  │
│                                                                                               │
│   dec_init_state: LSTMStateTuple(dec_c, dec_h)                                              │
│                                                                                               │
└──────────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Attention Mechanism

```
┌──────────────────────────────────────────────────────────────────────────────────────────────┐
│                              BAHDANAU ATTENTION                                               │
├──────────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                               │
│   INPUTS:                                                                                     │
│   • encoder_outputs: [1, 6, 512] (6 encoder positions)                                       │
│   • decoder_state: [1, 256] (current decoder hidden state)                                   │
│   • coverage: [1, 6] (cumulative attention, optional)                                        │
│                                                                                               │
│                                                                                               │
│   STEP 1: PROJECT DECODER STATE                                                               │
│   ────────────────────────────────                                                            │
│                                                                                               │
│   decoder_features = W_s × decoder_state + b_attn                                           │
│                    = [1, 256] @ [256, 512] + [512]                                          │
│                    = [1, 512]                                                                │
│                                                                                               │
│                                                                                               │
│   STEP 2: SCORE EACH ENCODER POSITION                                                         │
│   ─────────────────────────────────────                                                       │
│                                                                                               │
│   For each position i:                                                                        │
│                                                                                               │
│   e_i = v^T × tanh(encoder_features[i] + decoder_features + w_c × coverage[i])             │
│                                                                                               │
│   Position:     0        1        2        3        4        5                               │
│   Word:      Germany   beat   Argentina   in      the     final                             │
│                 │        │        │        │        │        │                               │
│                 ▼        ▼        ▼        ▼        ▼        ▼                               │
│   e_i:       [2.1]    [0.5]    [0.8]    [0.2]    [0.3]    [0.4]                            │
│                                                                                               │
│                                                                                               │
│   STEP 3: SOFTMAX                                                                             │
│   ───────────────                                                                             │
│                                                                                               │
│   α = softmax(e)                                                                             │
│                                                                                               │
│   Position:     0        1        2        3        4        5                               │
│   α:         [0.52]   [0.05]   [0.07]   [0.02]   [0.03]   [0.04]   (+ padding positions)   │
│               ████▌      █        █▌       ▌        ▌        █                              │
│                                                                                               │
│                                                                                               │
│   STEP 4: CONTEXT VECTOR                                                                      │
│   ──────────────────────                                                                      │
│                                                                                               │
│   context = Σ_i α_i × encoder_outputs[i]                                                    │
│                                                                                               │
│   Position 0: 0.52 × h_0 [512] = contribution_0                                             │
│   Position 1: 0.05 × h_1 [512] = contribution_1                                             │
│   ...                                                                                         │
│                                                                                               │
│   context = contribution_0 + contribution_1 + ... + contribution_5                          │
│   Shape: [1, 512]                                                                            │
│                                                                                               │
│   (Mostly influenced by h_0 since α_0 = 0.52)                                               │
│                                                                                               │
│                                                                                               │
│   VISUALIZATION:                                                                              │
│   ──────────────                                                                              │
│                                                                                               │
│   Decoder                                                                                     │
│   State    ─────────────────────────────────────────────────────▶ Query                      │
│                                                                     │                        │
│                                                                     │                        │
│   Encoder                                                           │                        │
│   Outputs: h_0   h_1   h_2   h_3   h_4   h_5 ─────────────────────▶ Keys                    │
│             │     │     │     │     │     │                         │                        │
│             │     │     │     │     │     │         ┌───────────────┘                        │
│             ▼     ▼     ▼     ▼     ▼     ▼         ▼                                        │
│           ┌───────────────────────────────────────────────┐                                  │
│           │        ATTENTION SCORES (e)                    │                                  │
│           │   e_0   e_1   e_2   e_3   e_4   e_5           │                                  │
│           └───────────────────┬───────────────────────────┘                                  │
│                               │ softmax                                                       │
│                               ▼                                                               │
│           ┌───────────────────────────────────────────────┐                                  │
│           │     ATTENTION WEIGHTS (α)                      │                                  │
│           │   0.52  0.05  0.07  0.02  0.03  0.04          │                                  │
│           └───────────────────┬───────────────────────────┘                                  │
│                               │ weighted sum                                                  │
│                               ▼                                                               │
│           ┌───────────────────────────────────────────────┐                                  │
│           │         CONTEXT VECTOR [512]                   │                                  │
│           └───────────────────────────────────────────────┘                                  │
│                                                                                               │
└──────────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Pointer-Generator

```
┌──────────────────────────────────────────────────────────────────────────────────────────────┐
│                            POINTER-GENERATOR MECHANISM                                        │
├──────────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                               │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                         P_GEN CALCULATION                                            │    │
│   │                                                                                      │    │
│   │   context_vector ──────────┐                                                        │    │
│   │         [512]              │                                                        │    │
│   │                            │                                                        │    │
│   │   decoder_state ───────────┼────────▶ [  Concat + Linear  ] ──▶ sigmoid ──▶ p_gen  │    │
│   │         [256]              │                                               [0.6]   │    │
│   │                            │                                                        │    │
│   │   input_embedding ─────────┘                                                        │    │
│   │         [128]                                                                       │    │
│   │                                                                                      │    │
│   └─────────────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                               │
│                                            │                                                  │
│                                            ▼                                                  │
│                                                                                               │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                        VOCABULARY DISTRIBUTION                                       │    │
│   │                                                                                      │    │
│   │   decoder_output ──▶ [Output Projection] ──▶ [Softmax] ──▶ vocab_dist              │    │
│   │       [256]            [256 × 50000]                        [50000]                │    │
│   │                                                                                      │    │
│   │   vocab_dist_weighted = p_gen × vocab_dist                                         │    │
│   │                       = 0.6 × vocab_dist                                           │    │
│   │                                                                                      │    │
│   │   Example:                                                                          │    │
│   │   vocab_dist["Germany"] = 0.45                                                     │    │
│   │   vocab_dist_weighted["Germany"] = 0.6 × 0.45 = 0.27                              │    │
│   │                                                                                      │    │
│   └─────────────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                               │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                          COPY DISTRIBUTION                                           │    │
│   │                                                                                      │    │
│   │   attention_dist ──▶ (1 - p_gen) × attn_dist ──▶ scatter to vocab indices          │    │
│   │       [enc_len]        = 0.4 × attn_dist                                           │    │
│   │                                                                                      │    │
│   │   Source: "Germany  beat  Argentina  in  the  final  Mario  Götze"                 │    │
│   │   IDs:      100     101     102      6    4   105   50000  50001                   │    │
│   │   α:       0.52    0.05    0.07   0.02  0.03  0.04   0.08   0.12                   │    │
│   │                                                                                      │    │
│   │   copy_dist[100] = 0.4 × 0.52 = 0.208  (Germany)                                   │    │
│   │   copy_dist[101] = 0.4 × 0.05 = 0.020  (beat)                                      │    │
│   │   copy_dist[102] = 0.4 × 0.07 = 0.028  (Argentina)                                 │    │
│   │   copy_dist[50000] = 0.4 × 0.08 = 0.032 (Mario - OOV!)                            │    │
│   │   copy_dist[50001] = 0.4 × 0.12 = 0.048 (Götze - OOV!)                            │    │
│   │                                                                                      │    │
│   └─────────────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                               │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                         FINAL DISTRIBUTION                                           │    │
│   │                                                                                      │    │
│   │   final_dist = vocab_dist_weighted + copy_dist                                      │    │
│   │                                                                                      │    │
│   │   final_dist[100] = 0.27 + 0.208 = 0.478  (Germany)                                │    │
│   │   final_dist[50001] = 0 + 0.048 = 0.048   (Götze - only from copy!)               │    │
│   │                                                                                      │    │
│   │   ┌────────────────────────────────────────────────────────────────────────────┐   │    │
│   │   │                    Extended Vocabulary [50002]                              │   │    │
│   │   │                                                                             │   │    │
│   │   │    0    1    2  ...  100  ...  50000   50001                               │   │    │
│   │   │  [PAD][UNK][START]... Ger ...  Mario   Götze                               │   │    │
│   │   │                       many                                                  │   │    │
│   │   │    ▬    ▬    ▬  ...  ████ ...   ██      ██                                 │   │    │
│   │   │                      0.478     0.032   0.048                                │   │    │
│   │   │                                                                             │   │    │
│   │   │   ◀── Regular Vocab [50000] ──▶◀── OOV [2] ──▶                            │   │    │
│   │   │                                                                             │   │    │
│   │   └────────────────────────────────────────────────────────────────────────────┘   │    │
│   │                                                                                      │    │
│   └─────────────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                               │
└──────────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Coverage Mechanism

```
┌──────────────────────────────────────────────────────────────────────────────────────────────┐
│                              COVERAGE MECHANISM                                               │
├──────────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                               │
│   Source: "Germany beat Argentina in the final"                                              │
│   Position:   0      1      2      3    4    5                                               │
│                                                                                               │
│                                                                                               │
│   STEP 1: Initialize coverage to zeros                                                        │
│   ─────────────────────────────────────                                                       │
│                                                                                               │
│   coverage_0 = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                                               │
│                                                                                               │
│   Attention at step 1: α_1 = [0.52, 0.05, 0.07, 0.02, 0.03, 0.04]                          │
│   (High attention on "Germany")                                                              │
│                                                                                               │
│                                                                                               │
│   STEP 2: Update coverage                                                                     │
│   ───────────────────────                                                                     │
│                                                                                               │
│   coverage_1 = coverage_0 + α_1                                                              │
│              = [0.52, 0.05, 0.07, 0.02, 0.03, 0.04]                                         │
│                  ↑                                                                            │
│               "Germany" now has high coverage                                                │
│                                                                                               │
│                                                                                               │
│   STEP 3: Coverage influences next attention                                                  │
│   ──────────────────────────────────────────────                                              │
│                                                                                               │
│   e_i = v × tanh(encoder_feat + decoder_feat + w_c × coverage[i])                          │
│                                                    ↑                                         │
│                                        NEGATIVE weight (learned)                             │
│                                                                                               │
│   If coverage[i] is high and w_c is negative:                                               │
│   → e_i decreases → lower attention probability                                             │
│                                                                                               │
│   This DISCOURAGES attending to already-covered positions!                                  │
│                                                                                               │
│                                                                                               │
│   VISUALIZATION OVER TIME:                                                                    │
│   ────────────────────────                                                                    │
│                                                                                               │
│   Position:      Ger   beat   Arg    in    the   final                                      │
│                                                                                               │
│   Step 1: α_1   ████▌   █      █▌     ▌      ▌      █                                       │
│           cov   ░░░░░   ░      ░░     ░      ░      ░                                        │
│                                                                                               │
│   Step 2: α_2    ██     █     ███     █      █      █                                       │
│           cov   ████░   ░░     ░░░    ░      ░      ░                                        │
│                  ↑                                                                            │
│              Coverage PREVENTS re-attending to "Germany"                                     │
│                                                                                               │
│   Step 3: α_3    █      █      ██     █     ███     █                                       │
│           cov   ████░   ░░    ███░    ░░    ░░░     ░                                        │
│                                             ↑                                                │
│                              Now attending to "the"                                         │
│                                                                                               │
│   Step 4: α_4    ▌      █      █      █      ██    ███                                      │
│           cov   ████░   ░░░   ███░    ░░    ███░   ░░░                                       │
│                                                     ↑                                        │
│                                      Now attending to "final"                               │
│                                                                                               │
│                                                                                               │
│   COVERAGE LOSS:                                                                              │
│   ──────────────                                                                              │
│                                                                                               │
│   L_cov = Σ_t Σ_i min(α_ti, coverage_ti)                                                   │
│                                                                                               │
│   This penalizes overlap between current attention and past coverage.                       │
│                                                                                               │
│   If model attends to position i with:                                                       │
│   • High α_ti (attending now)                                                               │
│   • High coverage_ti (attended before)                                                      │
│   → min(α, cov) is HIGH → HIGH penalty                                                      │
│                                                                                               │
│   This encourages the model to attend to NEW positions each step.                           │
│                                                                                               │
└──────────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Training Pipeline

```
┌──────────────────────────────────────────────────────────────────────────────────────────────┐
│                               TRAINING PIPELINE                                               │
├──────────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                               │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                              DATA FLOW                                               │    │
│   │                                                                                      │    │
│   │    TFRecord     ──▶    Example     ──▶    Example     ──▶     Batch                │    │
│   │    Files              Generator          Queue              Queue                   │    │
│   │                                                                                      │    │
│   │   *.bin                                [1600 examples]    [100 batches]             │    │
│   │                                                                                      │    │
│   │                    4 threads fill ──▶ ◀── 4 threads create batches                 │    │
│   │                                                                                      │    │
│   └─────────────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                               │
│                                            │                                                  │
│                                            ▼                                                  │
│                                                                                               │
│   ┌─────────────────────────────────────────────────────────────────────────────────────┐    │
│   │                            TRAINING LOOP                                             │    │
│   │                                                                                      │    │
│   │   while True:                                                                        │    │
│   │       │                                                                              │    │
│   │       ▼                                                                              │    │
│   │   ┌──────────────────────────────────────────────────────────────────────────┐      │    │
│   │   │  1. GET BATCH                                                             │      │    │
│   │   │                                                                           │      │    │
│   │   │  batch = batcher.next_batch()                                            │      │    │
│   │   │                                                                           │      │    │
│   │   │  Contains:                                                                │      │    │
│   │   │  • enc_batch [16, max_enc_len]                                           │      │    │
│   │   │  • dec_batch [16, max_dec_len]                                           │      │    │
│   │   │  • target_batch [16, max_dec_len]                                        │      │    │
│   │   │  • masks, OOV info, etc.                                                 │      │    │
│   │   └──────────────────────────────────────────────────────────────────────────┘      │    │
│   │       │                                                                              │    │
│   │       ▼                                                                              │    │
│   │   ┌──────────────────────────────────────────────────────────────────────────┐      │    │
│   │   │  2. FORWARD PASS                                                          │      │    │
│   │   │                                                                           │      │    │
│   │   │  batch ──▶ Encoder ──▶ Decoder ──▶ Output ──▶ Loss                       │      │    │
│   │   │                                                                           │      │    │
│   │   │  loss = L_NLL + λ × L_coverage                                           │      │    │
│   │   └──────────────────────────────────────────────────────────────────────────┘      │    │
│   │       │                                                                              │    │
│   │       ▼                                                                              │    │
│   │   ┌──────────────────────────────────────────────────────────────────────────┐      │    │
│   │   │  3. BACKWARD PASS                                                         │      │    │
│   │   │                                                                           │      │    │
│   │   │  gradients = ∂loss/∂params                                               │      │    │
│   │   │                                                                           │      │    │
│   │   │  gradients ──▶ clip_by_global_norm(max=2.0) ──▶ clipped_gradients        │      │    │
│   │   └──────────────────────────────────────────────────────────────────────────┘      │    │
│   │       │                                                                              │    │
│   │       ▼                                                                              │    │
│   │   ┌──────────────────────────────────────────────────────────────────────────┐      │    │
│   │   │  4. UPDATE WEIGHTS                                                        │      │    │
│   │   │                                                                           │      │    │
│   │   │  Adagrad optimizer:                                                       │      │    │
│   │   │  accumulator += gradient²                                                 │      │    │
│   │   │  param -= lr × gradient / √(accumulator + ε)                             │      │    │
│   │   │                                                                           │      │    │
│   │   │  lr = 0.15 (default)                                                     │      │    │
│   │   └──────────────────────────────────────────────────────────────────────────┘      │    │
│   │       │                                                                              │    │
│   │       ▼                                                                              │    │
│   │   ┌──────────────────────────────────────────────────────────────────────────┐      │    │
│   │   │  5. LOG & CHECKPOINT                                                      │      │    │
│   │   │                                                                           │      │    │
│   │   │  Every 100 steps: Write Tensorboard summary                              │      │    │
│   │   │  Every 60 secs:   Save checkpoint                                        │      │    │
│   │   └──────────────────────────────────────────────────────────────────────────┘      │    │
│   │                                                                                      │    │
│   └─────────────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                               │
└──────────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Beam Search

```
┌──────────────────────────────────────────────────────────────────────────────────────────────┐
│                                 BEAM SEARCH (K=4)                                             │
├──────────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                               │
│   STEP 0: Initialize with START token                                                         │
│   ─────────────────────────────────────                                                       │
│                                                                                               │
│   Beam 1: [START]  log_prob = 0.0                                                            │
│                                                                                               │
│                                                                                               │
│   STEP 1: Expand to top-K candidates                                                          │
│   ──────────────────────────────────────                                                      │
│                                                                                               │
│   [START] ──▶ Model ──▶ P(next word)                                                         │
│                                                                                               │
│   Top 4 candidates:                                                                           │
│   ┌───────────────────────────────────────────────────────────────────────────────────┐      │
│   │  Beam 1: [START, Germany]    log_prob = -0.69   (P=0.50)                          │      │
│   │  Beam 2: [START, The]        log_prob = -1.20   (P=0.30)                          │      │
│   │  Beam 3: [START, Argentina]  log_prob = -1.90   (P=0.15)                          │      │
│   │  Beam 4: [START, World]      log_prob = -3.00   (P=0.05)                          │      │
│   └───────────────────────────────────────────────────────────────────────────────────┘      │
│                                                                                               │
│                                                                                               │
│   STEP 2: Expand each beam, keep top-K overall                                                │
│   ─────────────────────────────────────────────                                               │
│                                                                                               │
│   [START, Germany] ──▶ Model ──▶ {won: 0.35, beat: 0.30, ...}                               │
│   [START, The]     ──▶ Model ──▶ {World: 0.40, match: 0.25, ...}                            │
│   [START, Argentina]──▶ Model ──▶ {lost: 0.30, ...}                                         │
│   [START, World]   ──▶ Model ──▶ {Cup: 0.50, ...}                                           │
│                                                                                               │
│   All candidates (cumulative log_prob):                                                       │
│   Germany + won:      -0.69 + -1.05 = -1.74                                                 │
│   Germany + beat:     -0.69 + -1.20 = -1.89                                                 │
│   The + World:        -1.20 + -0.92 = -2.12                                                 │
│   Argentina + lost:   -1.90 + -1.20 = -3.10                                                 │
│   World + Cup:        -3.00 + -0.69 = -3.69                                                 │
│   ...                                                                                         │
│                                                                                               │
│   Keep top 4:                                                                                 │
│   ┌───────────────────────────────────────────────────────────────────────────────────┐      │
│   │  Beam 1: [START, Germany, won]       log_prob = -1.74                             │      │
│   │  Beam 2: [START, Germany, beat]      log_prob = -1.89                             │      │
│   │  Beam 3: [START, The, World]         log_prob = -2.12                             │      │
│   │  Beam 4: [START, Argentina, lost]    log_prob = -3.10                             │      │
│   └───────────────────────────────────────────────────────────────────────────────────┘      │
│                                                                                               │
│                                                                                               │
│   CONTINUE UNTIL STOP TOKEN...                                                                │
│   ─────────────────────────────                                                               │
│                                                                                               │
│   Completed hypotheses:                                                                       │
│   ┌───────────────────────────────────────────────────────────────────────────────────┐      │
│   │  [START, Germany, won, the, Cup, STOP]           log_prob = -4.20                 │      │
│   │  [START, Germany, beat, Argentina, STOP]         log_prob = -4.85                 │      │
│   │  [START, The, World, Cup, goes, ..., STOP]       log_prob = -5.50                 │      │
│   └───────────────────────────────────────────────────────────────────────────────────┘      │
│                                                                                               │
│   Length normalization:                                                                       │
│   avg_log_prob = log_prob / length                                                           │
│                                                                                               │
│   [Germany, won, the, Cup]:        -4.20 / 5 = -0.84  ← BEST!                               │
│   [Germany, beat, Argentina]:      -4.85 / 4 = -1.21                                        │
│   [The, World, Cup, goes, ...]:    -5.50 / 8 = -0.69                                        │
│                                                                                               │
│   OUTPUT: "Germany won the Cup"                                                              │
│                                                                                               │
└──────────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Summary

This document provides visual reference for all major components:

1. **Architecture Overview**: Full system diagram
2. **Data Flow**: From raw data to output
3. **Encoder**: Bidirectional LSTM structure
4. **Attention**: Bahdanau attention computation
5. **Pointer-Generator**: p_gen and distribution combination
6. **Coverage**: Preventing repetition
7. **Training**: Complete pipeline
8. **Beam Search**: Decoding process

Use these diagrams as quick visual references when understanding or implementing the model.

---

*Next: [16_glossary.md](16_glossary.md) - Key Terms and Definitions*
